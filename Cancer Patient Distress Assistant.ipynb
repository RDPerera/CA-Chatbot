{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['how', 'the', 'fuk', 'who', 'the', 'heck', 'moved', 'my', 'fridge', 'should', 'i', 'knock', 'the', 'landlord', 'door', 'angry', 'mad'], 'anger')\n",
      "760\n",
      "995\n",
      "673\n",
      "714\n",
      "4689\n",
      "0\n",
      "['anger', 'fear', 'sadness', 'joy']\n",
      "(3751, 7580)\n",
      "(3751, 4)\n",
      "(938, 7580)\n",
      "(938, 4)\n",
      " Shape of X is  (7580, 3751)\n",
      " Shape of Y is  (4, 3751)\n",
      " Shape of m is  3751\n",
      " Shape of W1 is  (100, 7580)\n",
      " Shape of W2 is  (4, 100)\n",
      "################### TRAIN MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.25      0.35      2203\n",
      "           1       0.16      0.21      0.18       789\n",
      "           2       0.03      0.19      0.05       121\n",
      "           3       0.13      0.18      0.15       638\n",
      "\n",
      "    accuracy                           0.23      3751\n",
      "   macro avg       0.23      0.21      0.18      3751\n",
      "weighted avg       0.40      0.23      0.27      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.90      0.71       616\n",
      "           1       0.89      0.60      0.72      1567\n",
      "           2       0.54      0.75      0.63       628\n",
      "           3       0.71      0.69      0.70       940\n",
      "\n",
      "    accuracy                           0.69      3751\n",
      "   macro avg       0.69      0.73      0.69      3751\n",
      "weighted avg       0.74      0.69      0.70      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.68      0.78      1263\n",
      "           1       0.85      0.87      0.86      1026\n",
      "           2       0.68      0.92      0.78       633\n",
      "           3       0.80      0.88      0.84       829\n",
      "\n",
      "    accuracy                           0.82      3751\n",
      "   macro avg       0.81      0.84      0.82      3751\n",
      "weighted avg       0.83      0.82      0.82      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.98      0.83       686\n",
      "           1       0.94      0.74      0.83      1327\n",
      "           2       0.77      0.88      0.82       751\n",
      "           3       0.89      0.82      0.85       987\n",
      "\n",
      "    accuracy                           0.83      3751\n",
      "   macro avg       0.83      0.86      0.83      3751\n",
      "weighted avg       0.85      0.83      0.83      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75      1542\n",
      "           1       0.77      0.99      0.87       807\n",
      "           2       0.76      0.95      0.84       686\n",
      "           3       0.77      0.98      0.86       716\n",
      "\n",
      "    accuracy                           0.82      3751\n",
      "   macro avg       0.82      0.88      0.83      3751\n",
      "weighted avg       0.86      0.82      0.81      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77       582\n",
      "           1       0.98      0.65      0.78      1584\n",
      "           2       0.74      0.96      0.84       660\n",
      "           3       0.89      0.87      0.88       925\n",
      "\n",
      "    accuracy                           0.81      3751\n",
      "   macro avg       0.81      0.87      0.82      3751\n",
      "weighted avg       0.86      0.81      0.81      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.59      0.74      1590\n",
      "           1       0.75      1.00      0.86       781\n",
      "           2       0.77      0.95      0.85       693\n",
      "           3       0.75      0.99      0.85       687\n",
      "\n",
      "    accuracy                           0.81      3751\n",
      "   macro avg       0.81      0.88      0.82      3751\n",
      "weighted avg       0.86      0.81      0.80      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       613\n",
      "           1       0.98      0.70      0.82      1456\n",
      "           2       0.78      0.95      0.86       712\n",
      "           3       0.93      0.87      0.90       970\n",
      "\n",
      "    accuracy                           0.84      3751\n",
      "   macro avg       0.84      0.88      0.84      3751\n",
      "weighted avg       0.88      0.84      0.84      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.74      0.85      1244\n",
      "           1       0.79      1.00      0.88       834\n",
      "           2       0.87      0.88      0.88       849\n",
      "           3       0.88      0.97      0.92       824\n",
      "\n",
      "    accuracy                           0.88      3751\n",
      "   macro avg       0.88      0.90      0.88      3751\n",
      "weighted avg       0.89      0.88      0.88      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92       850\n",
      "           1       0.95      0.89      0.92      1114\n",
      "           2       0.87      0.92      0.89       814\n",
      "           3       0.96      0.90      0.93       973\n",
      "\n",
      "    accuracy                           0.92      3751\n",
      "   macro avg       0.91      0.92      0.92      3751\n",
      "weighted avg       0.92      0.92      0.92      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92      1068\n",
      "           1       0.90      0.98      0.93       958\n",
      "           2       0.88      0.93      0.91       817\n",
      "           3       0.94      0.94      0.94       908\n",
      "\n",
      "    accuracy                           0.92      3751\n",
      "   macro avg       0.92      0.93      0.92      3751\n",
      "weighted avg       0.93      0.92      0.92      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       954\n",
      "           1       0.93      0.94      0.94      1035\n",
      "           2       0.90      0.94      0.92       821\n",
      "           3       0.96      0.93      0.94       941\n",
      "\n",
      "    accuracy                           0.93      3751\n",
      "   macro avg       0.93      0.93      0.93      3751\n",
      "weighted avg       0.94      0.93      0.94      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94      1013\n",
      "           1       0.93      0.96      0.94      1007\n",
      "           2       0.89      0.95      0.92       808\n",
      "           3       0.96      0.94      0.95       923\n",
      "\n",
      "    accuracy                           0.94      3751\n",
      "   macro avg       0.94      0.94      0.94      3751\n",
      "weighted avg       0.94      0.94      0.94      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95       984\n",
      "           1       0.94      0.96      0.95      1028\n",
      "           2       0.90      0.95      0.92       814\n",
      "           3       0.96      0.94      0.95       925\n",
      "\n",
      "    accuracy                           0.94      3751\n",
      "   macro avg       0.94      0.94      0.94      3751\n",
      "weighted avg       0.94      0.94      0.94      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       998\n",
      "           1       0.94      0.96      0.95      1022\n",
      "           2       0.90      0.96      0.93       813\n",
      "           3       0.96      0.95      0.96       918\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       988\n",
      "           1       0.94      0.96      0.95      1027\n",
      "           2       0.91      0.96      0.93       818\n",
      "           3       0.96      0.96      0.96       918\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       993\n",
      "           1       0.95      0.96      0.95      1028\n",
      "           2       0.91      0.96      0.94       817\n",
      "           3       0.96      0.96      0.96       913\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96       987\n",
      "           1       0.95      0.97      0.96      1032\n",
      "           2       0.92      0.97      0.94       816\n",
      "           3       0.97      0.96      0.97       916\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96       985\n",
      "           1       0.96      0.97      0.96      1032\n",
      "           2       0.92      0.97      0.94       818\n",
      "           3       0.97      0.97      0.97       916\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       983\n",
      "           1       0.96      0.97      0.96      1034\n",
      "           2       0.93      0.97      0.95       822\n",
      "           3       0.97      0.97      0.97       912\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       982\n",
      "           1       0.96      0.97      0.96      1035\n",
      "           2       0.93      0.97      0.95       824\n",
      "           3       0.97      0.97      0.97       910\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       979\n",
      "           1       0.96      0.97      0.97      1034\n",
      "           2       0.93      0.97      0.95       826\n",
      "           3       0.98      0.97      0.98       912\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.97      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       975\n",
      "           1       0.96      0.97      0.97      1037\n",
      "           2       0.94      0.97      0.95       828\n",
      "           3       0.98      0.98      0.98       911\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       971\n",
      "           1       0.97      0.97      0.97      1038\n",
      "           2       0.94      0.97      0.95       829\n",
      "           3       0.98      0.98      0.98       913\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       971\n",
      "           1       0.97      0.97      0.97      1039\n",
      "           2       0.94      0.97      0.96       828\n",
      "           3       0.98      0.98      0.98       913\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       970\n",
      "           1       0.97      0.97      0.97      1040\n",
      "           2       0.94      0.97      0.96       830\n",
      "           3       0.98      0.98      0.98       911\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       970\n",
      "           1       0.97      0.98      0.97      1038\n",
      "           2       0.95      0.97      0.96       833\n",
      "           3       0.98      0.98      0.98       910\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       970\n",
      "           1       0.97      0.98      0.98      1037\n",
      "           2       0.95      0.97      0.96       835\n",
      "           3       0.98      0.98      0.98       909\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       967\n",
      "           1       0.97      0.98      0.98      1038\n",
      "           2       0.95      0.97      0.96       837\n",
      "           3       0.98      0.98      0.98       909\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       964\n",
      "           1       0.97      0.98      0.98      1039\n",
      "           2       0.95      0.98      0.96       837\n",
      "           3       0.98      0.98      0.98       911\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       961\n",
      "           1       0.98      0.98      0.98      1039\n",
      "           2       0.96      0.98      0.97       841\n",
      "           3       0.98      0.98      0.98       910\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       959\n",
      "           1       0.98      0.98      0.98      1040\n",
      "           2       0.96      0.98      0.97       841\n",
      "           3       0.99      0.98      0.99       911\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       958\n",
      "           1       0.98      0.98      0.98      1040\n",
      "           2       0.96      0.98      0.97       842\n",
      "           3       0.99      0.98      0.99       911\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       956\n",
      "           1       0.98      0.98      0.98      1040\n",
      "           2       0.96      0.98      0.97       845\n",
      "           3       0.99      0.99      0.99       910\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       955\n",
      "           1       0.98      0.98      0.98      1041\n",
      "           2       0.96      0.98      0.97       845\n",
      "           3       0.99      0.99      0.99       910\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       956\n",
      "           1       0.98      0.98      0.98      1042\n",
      "           2       0.96      0.98      0.97       844\n",
      "           3       0.99      0.99      0.99       909\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       956\n",
      "           1       0.98      0.99      0.98      1041\n",
      "           2       0.96      0.98      0.97       845\n",
      "           3       0.99      0.99      0.99       909\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       954\n",
      "           1       0.98      0.99      0.99      1040\n",
      "           2       0.97      0.98      0.97       848\n",
      "           3       0.99      0.99      0.99       909\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       954\n",
      "           1       0.98      0.99      0.99      1039\n",
      "           2       0.97      0.98      0.97       849\n",
      "           3       0.99      0.99      0.99       909\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.99      1038\n",
      "           2       0.97      0.98      0.98       851\n",
      "           3       0.99      0.99      0.99       908\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.99      1039\n",
      "           2       0.97      0.98      0.98       851\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.99      1039\n",
      "           2       0.97      0.98      0.98       851\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       952\n",
      "           1       0.98      0.99      0.99      1040\n",
      "           2       0.97      0.98      0.98       852\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       951\n",
      "           1       0.98      0.99      0.99      1038\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       951\n",
      "           1       0.98      0.99      0.99      1039\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       906\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       948\n",
      "           1       0.99      0.99      0.99      1041\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       947\n",
      "           1       0.99      0.99      0.99      1041\n",
      "           2       0.98      0.98      0.98       856\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       947\n",
      "           1       0.99      0.99      0.99      1042\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       947\n",
      "           1       0.99      0.99      0.99      1042\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       947\n",
      "           1       0.99      0.99      0.99      1042\n",
      "           2       0.98      0.98      0.98       855\n",
      "           3       0.99      0.99      0.99       907\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "saved synapses to: weights.json\n",
      " Shape of X is  (7580, 938)\n",
      " Shape of Y is  (4, 938)\n",
      " Shape of m is  938\n",
      "################### TEST MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       245\n",
      "           1       0.93      0.94      0.93       292\n",
      "           2       0.87      0.92      0.89       189\n",
      "           3       0.97      0.95      0.96       212\n",
      "\n",
      "    accuracy                           0.93       938\n",
      "   macro avg       0.93      0.93      0.93       938\n",
      "weighted avg       0.93      0.93      0.93       938\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "\n",
    "anger_test_set = []\n",
    "fear_test_set = []\n",
    "sadness_test_set = []\n",
    "joy_test_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]\n",
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(\"./datasets/\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "\n",
    "def load_test_data(sentiment):\n",
    "    data = open(\"./datasets/\"+sentiment+\"_test_set.txt\",encoding=\"utf8\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        if line.split()[-1] == \"none\":\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-2]\n",
    "            training_set.append((sentence,label))\n",
    "        else:\n",
    "            intensity = float(line.split()[-1])        \n",
    "            if (intensity>=threshold):\n",
    "                line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "                punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "                result = line.translate(punct)\n",
    "                tokened_sentence = nltk.word_tokenize(result)\n",
    "                sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "                label = tokened_sentence[-1]\n",
    "                training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset, finds their stem and \n",
    "# encodes each sentence according to the bag of words appending it to training set\n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    all_test_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    labels = []\n",
    "    test_labels = []\n",
    "    words=[]\n",
    "    test_words = []\n",
    "        \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    print(anger_training_set[0])\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    \n",
    "    ######### Here we read the whole test data for each class and the threshold we will use for its classification\n",
    "    anger_test_data = load_test_data(\"anger\")\n",
    "    anger_test_set = clean_data(anger_test_data,threshold)\n",
    "    #print(anger_test_set[0])\n",
    "    print(len(anger_test_set))\n",
    "    \n",
    "    fear_test_data = load_test_data(\"fear\")\n",
    "    fear_test_set = clean_data(fear_test_data,threshold)\n",
    "   # print(fear_test_set[0])\n",
    "    print(len(fear_test_set))\n",
    "    \n",
    "    sadness_test_data = load_test_data(\"sadness\")\n",
    "    sadness_test_set = clean_data(sadness_test_data,threshold)\n",
    "  #  print(sadness_test_set[0])\n",
    "    print(len(sadness_test_set))\n",
    "    \n",
    "    joy_test_data = load_test_data(\"joy\")\n",
    "    joy_test_set = clean_data(joy_test_data,threshold)\n",
    "  #  print(joy_test_set[0])\n",
    "    print(len(joy_test_set))\n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    all_data.extend(anger_test_set)\n",
    "    all_data.extend(fear_test_set)\n",
    "    all_data.extend(sadness_test_set)\n",
    "    all_data.extend(joy_test_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    \n",
    "    \n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(len(labels))\n",
    "    print(len(test_labels))\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training,test set and training, test labels in a numpy array as it is required for calculations in neural net\n",
    "    dataset = np.array(training_set)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(dataset.shape[0])\n",
    "    shuffled_dataset, shuffled_labels = np.zeros((dataset.shape)),np.zeros((dataset.shape))\n",
    "    shuffled_dataset,shuffled_labels = dataset[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    \n",
    "    split = int(len(shuffled_dataset)*0.8)\n",
    "    training_data = shuffled_dataset[:split]\n",
    "    training_labels = shuffled_labels[:split]\n",
    "    test_data = shuffled_dataset[split:]\n",
    "    test_labels = shuffled_labels[split:]\n",
    "    print(training_data.shape)\n",
    "    print(training_labels.shape)    \n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "        \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(training_data,training_labels,words,classes)\n",
    "    Test_model(test_data,test_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Test_model(test_data, test_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = test_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = test_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    \n",
    "    weights_file = 'weights.json' \n",
    "    with open(weights_file) as data_file: \n",
    "        weights = json.load(data_file) \n",
    "        W1 = np.asarray(weights['weight1']) \n",
    "        W2 = np.asarray(weights['weight2'])\n",
    "        b1 = np.asarray(weights['bias1']) \n",
    "        b2 = np.asarray(weights['bias2'])\n",
    "\n",
    "    print(\"################### TEST MODEL STATISTICS ######################\")\n",
    "    for i in range(1):\n",
    "        # input layer is our encoded sentence\n",
    "        l0 = X\n",
    "        # matrix multiplication of input and hidden layer\n",
    "        l1 = relu(np.dot(W1,l0)+b1)\n",
    "        # output layer\n",
    "        l2 = softmax(np.dot(W2,l1)+b2)\n",
    "        predictions = np.argmax(l2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    print(\"################### TRAIN MODEL STATISTICS ######################\")\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to kill everyone,no one care about me \n",
      " classification: [['anger', array([0.50577153])], ['fear', array([0.23068252])], ['sadness', array([0.15631584])], ['joy', array([0.10723011])]] \n",
      "\n",
      "I am so happy about my health condition \n",
      " classification: [['joy', array([0.59867044])], ['anger', array([0.23261913])]] \n",
      "\n",
      "This depression will kill me someday .. i am dying @Name3 #killme \n",
      " classification: [['sadness', array([0.68405595])], ['fear', array([0.17246384])]] \n",
      "\n",
      "I am afraid of this cancer  @Name4 #cancer \n",
      " classification: [['fear', array([0.60974879])], ['sadness', array([0.16456603])], ['joy', array([0.12180752])], ['anger', array([0.10387766])]] \n",
      "\n",
      "What should I do when i am happy @Name5  \n",
      " classification: [['joy', array([0.2670336])], ['anger', array([0.26362993])], ['fear', array([0.24204206])], ['sadness', array([0.2272944])]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['joy', array([0.2670336])],\n",
       " ['anger', array([0.26362993])],\n",
       " ['fear', array([0.24204206])],\n",
       " ['sadness', array([0.2272944])]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.1\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "    \n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "#    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s \\n\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"I want to kill everyone,no one care about me\")\n",
    "classify(\"I am so happy about my health condition\")\n",
    "classify(\"This depression will kill me someday .. i am dying @Name3 #killme\")\n",
    "classify(\"I am afraid of this cancer  @Name4 #cancer\")\n",
    "classify(\"What should I do when i am happy @Name5 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
